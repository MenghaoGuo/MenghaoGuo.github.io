<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Meng-Hao Guo</title>
    <base href="https://menghaoguo.github.io/publication.html">
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<h1 style="padding-left: 0.5em">Meng-Hao Guo (Ph.D student at Tsinghua University)</h1><hr>
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
    <div class="menu-item"><a href="publication.html">Publications</a></div>
    <div class="menu-item"><a href="group.html">Group</a></div>
    <!-- <div class="menu-item"><a href="software.html">Software</a></div> -->
    <div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">

     <h1 style="margin-top: 0em">Publications</a></h1><br>
    <p>* indicates equal contributions. For details, please refer to google scholar.</p>

    <div>
        <h2><hr>Preprints</h2>
        <ul>
            <li><p>
                Agentic Keyframe Search for Video Question Answering <br>
                Sunqi Fan, <b>Meng-Hao Guo</b>, Shuojin Yang<br>
                [<A HREF="https://arxiv.org/pdf/2503.16032">PDF</A>]
            </p></li>
        </ul>
    </div>

    <div>
        <h2><hr><a name="conference"></a>Conference Papers</h2>
        <ol>
            <li><p>
                R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation. ICML 2025 <br>
                <b>Meng-Hao Guo</b>, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu <br>
                [<A HREF="https://arxiv.org/pdf/2505.02018">PDF</A>]
            </p></li>
            <li><p>
                One Model to Rig Them All: Diverse Skeleton Rigging with Unirig. SIGGRAPH 2025 (ACM TOG) <br>
                Jia-Peng Zhang, Cheng-Feng Pu, <b>Meng-Hao Guo</b>, Yan-Pei Cao, Shi-Min Hu <br>
                [<A HREF="https://zjp-shadow.github.io/works/UniRig/">Project Page</A>]
            </p></li>
            <li><p>
                Adaptive Parameter Selection for Tuning Vision-Language Models. CVPR 2025 <br>
                Yi Zhang, Yi-Xuan Deng, <b>Meng-Hao Guo</b>, Shi-Min Hu <br>
                [<A HREF="">PDF</A>]
            </p></li>
            <li><p>
                CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization. SIGGRAPH 2024 (ACM TOG) <br>
                Hao-Yang Peng, Jia-Peng Zhang, <b>Meng-Hao Guo</b>, Yan-Pei Cao, Shi-Min Hu <br>
                [<A HREF="https://dl.acm.org/doi/pdf/10.1145/3658217">PDF</A>]
            </p></li>
            <li><p>
                Exploring regional clues in CLIP for zero-shot semantic segmentation. CVPR 2024 <br>
                Yi Zhang, <b>Meng-Hao Guo</b>, Miao Wang, Shi-Min Hu <br>
                [<A HREF="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_Regional_Clues_in_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2024_paper.pdf">PDF</A>]
            </p></li>
            <li><p>
                Long Range Pooling for 3D Large-Scale Scene Understanding. CVPR 2023 <br>
                Xiang-Li Li, <b>Meng-Hao Guo</b>, Tai-Jiang Mu, Ralph R Martin, Shi-Min Hu <br>
                [<A HREF="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Long_Range_Pooling_for_3D_Large-Scale_Scene_Understanding_CVPR_2023_paper.pdf">PDF</A>]
            </p></li>
            <li><p>
                SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation. NeurIPS 2022 <br>
                <b>Meng-Hao Guo</b>, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, Shi-Min Hu <br>
                [<A HREF="https://proceedings.neurips.cc/paper_files/paper/2022/file/08050f40fff41616ccfc3080e60a301a-Paper-Conference.pdf">PDF</A>][<A HREF="https://github.com/Visual-Attention-Network/SegNeXt">CODE</A>]
            </p></li>
            <li><p>
                Is Attention Better Than Matrix Decomposition? ICLR 2021 <br>
                Zhengyang Geng*, <b>Meng-Hao Guo*</b>, Hongxu Chen, Xia Li, Ke Wei, Zhouchen Lin.<br>
                [<A HREF="https://openreview.net/pdf?id=1FvkSpWosOl">PDF</A>][<A HREF="https://github.com/Gsunshine/Enjoy-Hamburger">CODE</A>]
            </p></li>
            <li><p>
                Semi-supervised brain lesion segmentation with an adapted mean teacher model. IPMI 2019<br>
                Wenhui Cui, Yanlin Liu, Yuxing Li, <b>Menghao Guo</b>, Yiming Li, Xiuli Li, Tianle Wang, Xiangzhu Zeng, Chuyang Ye <br>
                [<A HREF="https://arxiv.org/pdf/1903.01248.pdf">PDF</A>]
            </p></li>
        </ol>
    </div>
    
    <div>
        <h2><hr><a name="journal"></a>Journal Papers</h2>
        <ol>
            <li><p>
                Tuning Vision-Language Models With Multiple Prototypes Clustering. T-PAMI 2024 <br>
                <b>Meng-Hao Guo</b>, Yi Zhang, Tai-Jiang Mu, Sharon X Huang, Shi-Min Hu.<br>
                [<a href="https://cg.cs.tsinghua.edu.cn/papers/PAMI-2024-TuningVLM.pdf" target="_blank">PDF</a>]
            </p></li>
            <li><p>
                Visual attention network. <strong style="color:red;">CVMJ 2023 Best Paper </strong> <br>
                <b>Meng-Hao Guo</b>, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.<br>
                [<a href="https://link.springer.com/article/10.1007/s41095-023-0364-2" target="_blank">PDF</a>][<A HREF="https://github.com/Visual-Attention-Network">CODE</A>]
            </p></li>
            <li><p>
                Beyond self-attention: External attention using two linear layers for visual tasks. T-PAMI <br>
                <b>Meng-Hao Guo</b>, Zheng-Ning Liu, Tai-Jiang Mu, Shi-Min Hu <br>
                [<a href="https://arxiv.org/pdf/2105.02358.pdf" target="_blank">PDF</a>][<A HREF="https://github.com/MenghaoGuo/EANet">CODE</A>]
            </p></li>
            <li><p>
                MWFormer: Mesh Understanding with window-based transformer. T-PAMI <br>
                Hao-Yang Peng, <b>Meng-Hao Guo</b>, Zheng-Ning Liu, Yong-Liang Yang, Tai-Jiang Mu <br>
                [<a href="http://yongliangyang.net/docs/mwformer_c&g23.pdf" target="_blank">PDF</a>]
            </p></li>
            <li><p>
                Attention mechanisms in computer vision: A survey.  <strong style="color:red;">CVMJ 2022 Best Paper </strong> <br>
                <b>Meng-Hao Guo</b>b>, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R Martin, Ming-Ming Cheng, Shi-Min Hu <br>
                [<a href="https://link.springer.com/article/10.1007/S41095-022-0271-Y" target="_blank">PDF</a>]
            </p></li>
            <li><p>
                Subdivision-Based Mesh Convolution Networks. ACM TOG 2022<br>
                Shi-Min Hu, Zheng-Ning Liu, <b>Meng-Hao Guo</b>, Jun-Xiong Cai, Jiahui Huang, Tai-Jiang Mu, Ralph R Martin <br>
                [<a href="https://arxiv.org/pdf/2106.00455.pdf" target="_blank">PDF</a>][<A HREF="https://github.com/lzhengning/SubdivNet">CODE</A>]
            </p></li>            
            <li><p>
                Pct: Point cloud transformer. CVMJ 2021 <br>
                <b>Meng-Hao Guo</b>, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, Shi-Min Hu <br>
                [<A HREF="https://arxiv.org/pdf/2012.09688.pdf">PDF</A>][<A HREF="https://github.com/MenghaoGuo/PCT">CODE</A>]
            </p></li>
            <li><p>
                Can Attention Enable MLPs To Catch Up With CNNs? CVMJ 2021 <br>
                <b>Meng-Hao Guo</b>, Zheng-Ning Liu, Tai-Jiang Mu, Dun Liang, Ralph R Martin, Shi-Min Hu. <br>
                [<A HREF="https://arxiv.org/pdf/2105.15078.pdf">PDF</A>]
            </p></li>
        </ol>
    </div>

    
    
</td>
</tr>
</table>
</body>
</html>
